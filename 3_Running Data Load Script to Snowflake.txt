Install libraries outside virtual environment as well

pip install pandas
pip install "snowflake-connector-python[pandas]"(preferably this one)  or pip install snowflake-connector-python
pip install boto3

https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.html

Adding Snowflake parameters to the AWS parameter store.


Adding permissions to the EC2 Instance IAM Role


Creating Airflow DAG with s3 sensor to load data as soon as it is available in source path.

#####Start of script

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.bash_operator import BashOperator
##This used to load a script from a different directory
##In our use case the script to load data into snowflake is located in a subfolder inside the dags folder

import sys
sys.path.append('/home/airflow/airflow-code-demo/dags')
from source_load.data_load import run_script





default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 5, 12),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    dag_id='Netflix_Data_Analytics',
    default_args=default_args,
    description='This dag runs data analytics on top of netflix datasets',
    schedule_interval=timedelta(days=1),
)

credits_sensor = S3KeySensor(
    task_id='credits_rawfile_sensor',
    poke_interval=60 * 5,
    timeout=60 * 60 * 24 * 7,
    bucket_key='raw_files/credits.csv',
    wildcard_match=True,
    bucket_name='netflix-data-analytics',
    aws_conn_id='aws_default',
    dag=dag
)

titles_sensor = S3KeySensor(
    task_id='titles_rawfile_sensor',
    poke_interval=60 * 5,
    timeout=60 * 60 * 24 * 7,
    bucket_key='raw_files/titles.csv',
    wildcard_match=True,
    bucket_name='netflix-data-analytics',
    aws_conn_id='aws_default',
    dag=dag
)

load_data_snowflake = PythonOperator(task_id='Load_Data_Snowflake'
    ,python_callable=run_script, 
    dag=dag)
    
run_stage_model = BashOperator(
    task_id='run_staging_models',
    bash_command='dbt run --models tag:your_tag_name --profile your_profile --target your_target',
    dag=dag
)

start_task = DummyOperator(task_id='start_task', dag=dag)
end_task = DummyOperator(task_id='end_task', dag=dag)

start_task >> credits_sensor >> titles_sensor >> load_data_snowflake  >> end_task

##End of script

Create another folder in dags by the name of source_load and add the below python script as data_load.py

####Start of Script#######
import pandas as pd

# Snowflake connector libraries
import snowflake.connector as snow
from snowflake.connector.pandas_tools import write_pandas
import boto3

ssm = boto3.client('ssm',region_name='us-east-1')
s3 = boto3.client('s3',region_name='us-east-1')

## Please ensure these parameters are added in the AWS Systems Manager and the EC2 instance IAM has access to s3 and the ssm services
sf_username = ssm.get_parameter(Name='/snowflake/username', WithDecryption=True)['Parameter']['Value']
sf_password = ssm.get_parameter(Name='/snowflake/password', WithDecryption=True)['Parameter']['Value']
sf_account = ssm.get_parameter(Name='/snowflake/accountname', WithDecryption=True)['Parameter']['Value']

##If you have "https://my-dummy-organisation.snowflakecomputing.com/console/login" as your snowflake login url then the account_name is my-dummy-organisation

def run_script():

   #Module to create the snowflake connection and return the connection objects
   def create_connection():
      conn = snow.connect(user=sf_username,
      password=sf_password,
      account=sf_account,
      warehouse="COMPUTE_WH",
      database="PROD",
      schema="DBT_RAW")
      cursor = conn.cursor()
      print('SQL Connection Created')
      return cursor,conn

   # Module to truncate the table if exists. This will ensure duplicate load doesn't happen
   def truncate_table():
      cur,conn=create_connection()
      sql_titles = "TRUNCATE TABLE IF EXISTS TITLES_RAW"
      sql_credits = "TRUNCATE TABLE IF EXISTS CREDITS_RAW"
      cur.execute(sql_titles)
      cur.execute(sql_credits)
      print('Tables truncated')

   #Module to read csv file and load data in Snowflake. Table is created dynamically
   def load_data():
      titles_file = s3.get_object(Bucket='netflix-data-analytics', Key='raw_files/titles.csv')
      credits_file = s3.get_object(Bucket='netflix-data-analytics', Key='raw_files/credits.csv')
      
      cur,conn=create_connection()
      #titles_file = r"C:/Users/Aditya/OneDrive/Desktop/dbt_Training/Netflix_Dataset/titles.csv" # <- Replace with your path.
      delimiter = "," # Replace if you're using a different delimiter.
      #credits_file=r"C:/Users/Aditya/OneDrive/Documents/GitHub/dbt-code/datasets/credits.csv"

      titles_df = pd.read_csv(titles_file['Body'], sep = delimiter)
      print("Titles file read")
      credits_df = pd.read_csv(credits_file['Body'], sep = delimiter)
      print("Credits file read")

      write_pandas(conn, titles_df, "TITLES",auto_create_table=True)
      print('Titles file loaded')
      write_pandas(conn, credits_df, "CREDITS",auto_create_table=True)
      print('Credits file loaded')

      cur = conn.cursor()


      # Close your cursor and your connection.
      cur.close()
      conn.close()

   print("Starting Script")
   truncate_table()
   load_data()
